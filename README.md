# Web_scraping
## 认识爬虫的基本原理
- 信息来源：https://mp.weixin.qq.com/s/aroc__BtSCqk6o_StKQkWA
### 一、爬虫的基本原理
通俗地说，爬虫的工作流程可以分为几个步骤：

发送请求：就像在浏览器输入网址按下回车，爬虫会用requests、httpx 等库向目标网站发出 HTTP 请求。
获取响应：服务器返回结果，可能是 HTML、JSON，或其他格式的数据。
解析内容：程序从响应中提取需要的信息，比如标题、价格、评论。
数据存储：最终把抓到的数据保存到 CSV、Excel，或者数据库（MySQL、MongoDB）中。

简单一句话：爬虫 = 请求网页 → 提取数据 → 保存结果。

要理解上面这些步骤，先要弄清楚浏览器访问网页时发生了什么：
当你在浏览器输入网址，浏览器会发出一个HTTP请求。
服务器返回响应，里面包含状态码（200成功、404未找到、429请求过多）、响应头（数据类型、编码信息）和响应体（HTML、JSON或文件）。
爬虫所做的，就是模拟浏览器发请求，再读取响应。

这里还有两个常见情况：
静态页面：HTML里就有完整数据，用requests抓下来就能解析。
动态页面：页面先加载框架，再通过JavaScript请求后台接口拿数据，这时候要么用浏览器开发者工具找到接口地址，要么用自动化工具抓取。

### 二、Python爬虫常用技术栈

1. 网络请求
requests：最常用的同步请求库，语法简单，适合新手。
httpx/aiohttp：支持异步并发，可以一次性抓取成百上千个页面，大大提高效率。
为什么异步能更快？
因为爬虫大部分时间在“等服务器返回”。同步请求时，一个页面没回来，下一个页面就卡住了；异步模式下，能同时发出多个请求，“一边等一边干别的”，效率更高。

2. 网页解析相关
BeautifulSoup：适合新手，写法直观。
lxml：速度快，支持XPath语法。
re（正则表达式）：处理简单规则的数据提取。
json 模块：如果接口返回的是JSON格式，直接用即可。
👉 技能点：熟悉HTML标签结构，掌握XPath或CSS选择器。

3. 反爬虫应对
网站为了防止爬虫，大多会设置反爬策略，例如：
限制访问频率
检测User-Agent
要求登录/验证码
返回动态加载数据

对应的解决方案包括：
添加headers（模拟浏览器请求）
使用代理IP池（kookeey提供的动态住宅代理，更好模拟真实用户访问，降低封禁风险）
处理JS渲染

4. 数据存储
CSV / Excel：简单保存，适合小规模数据。
MySQL / PostgreSQL：关系型数据库，适合结构化数据。
MongoDB：非关系型数据库，灵活度更高。

5. 并发与调度
多线程 / 多进程：提高抓取效率。
异步编程（asyncio + aiohttp）：更高效的方案。
爬虫框架Scrapy：自带调度、去重、管道，适合大型项目。

网站往往会不断升级防护措施，而开发者也需要优化策略。

如果你用单一 IP 不断请求，容易被封。
使用代理IP（例如静态住宅代理、动态旋转代理），可以模拟真实用户访问，让爬虫更隐蔽。
对于需要账号登录的平台，还可能涉及Cookie、Token、Session的维护。

这就是为什么很多跨境电商、数据分析公司都会配合专业代理服务使用爬虫，确保账号与业务数据的稳定。

## 我的爬虫项目：微博、豆瓣以及北大法宝
### weibo爬虫
微博的爬虫已经很成熟了了，直接从git上克隆项目修改后就可以使用，比如：以“weiboSpider”为关键词进行检索这个部然后自己补充cookies调整一下保存的内容和目录即可。
这个部分内容本来是想用在博士毕业论文中通过词向量衡量老年歧视的，但是最终没有使用，但是仍然非常感谢我的师弟[ghmzhu](https://ghmzhu.github.io/)在2024年的暑假一直指导和帮忙我这件事。
我的这个[微博爬虫](https://github.com/Lingjun-Liu/Web_scraping/weibo-search-master2)主要是根据关键词爬取所有的热搜标题，再根据全部的标题list来爬取相应的博文内容。
使用了[程序员阿江](https://github.com/NanmiCoder/CrawlerTutorial)的内容.

