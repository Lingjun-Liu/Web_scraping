# 🕸️ Web Scraping Project

本项目旨在系统展示我在博士期间对网页爬虫技术的学习、实践与反思。内容涵盖爬虫的基本原理、常用技术栈，以及我用的三个代表性项目：微博爬虫、豆瓣小组爬虫与北大法宝法律文献爬虫。

---

## 一、网页爬虫的基本原理与常见技术

> 参考资料：[理解网络爬虫的原理与实现](https://mp.weixin.qq.com/s/aroc__BtSCqk6o_StKQkWA)

### 1️⃣ 爬虫的工作原理

网页爬虫（Web Crawler / Scraper）是一种模拟浏览器行为、自动化获取网页信息的程序。其核心流程包括：

1. **发送请求**：使用如 `requests`、`httpx` 等库向目标网站发出 HTTP 请求；
2. **获取响应**：服务器返回 HTML、JSON 或其他格式的数据；
3. **解析内容**：利用解析库提取所需字段，如标题、评论、价格等；
4. **数据存储**：将结果保存为 CSV、Excel 或数据库（如 MySQL、MongoDB）。

简言之：

> 🧩 **爬虫 = 请求网页 → 提取数据 → 保存结果。**

浏览器访问网页时，会经历“请求（Request）—响应（Response）”的完整过程。爬虫所做的，就是自动化地复现这一过程。

常见两种页面类型：
- **静态页面**：数据直接嵌在 HTML 中，可通过简单请求获取；
- **动态页面**：需通过 JavaScript 异步请求加载，需借助浏览器开发者工具或自动化工具（如 Selenium）分析接口。

---

### 2️⃣ Python 爬虫常用技术栈

#### （1）网络请求
- `requests`：同步请求库，语法简洁，入门友好；
- `httpx` / `aiohttp`：支持异步并发，适合大规模高效抓取。

> 💡 **异步的优势**：爬虫的主要耗时在“等待响应”，异步允许同时发送多个请求，实现“一边等待，一边工作”。

#### （2）网页解析
- `BeautifulSoup`：易上手、语法直观；
- `lxml`：解析速度快，支持 XPath；
- `re`：用于简单规则提取；
- `json`：用于接口返回的结构化数据。

> 技能要点：理解 HTML DOM 结构，熟悉 XPath 或 CSS 选择器语法。

#### （3）反爬策略与应对
网站常见的反爬措施包括：
- 访问频率限制；
- User-Agent 与 Cookie 检测；
- 登录验证或验证码机制；
- 动态渲染与数据分片。

对应的常用应对策略：
- 自定义 `headers` 模拟浏览器访问；
- 使用代理 IP 池以分散请求；
- 借助自动化浏览器（如 `Selenium`）执行 JS 渲染；
- 控制访问间隔，模拟“正常用户行为”。

#### （4）数据存储方式
- **CSV / Excel**：轻量、便于快速预览；
- **MySQL / PostgreSQL**：适合结构化研究数据；
- **MongoDB**：灵活存储非结构化文本。

#### （5）并发与调度
- 多线程 / 多进程：适合中小规模爬取；
- 异步编程（`asyncio` + `aiohttp`）：更高并发效率；
- 爬虫框架：`Scrapy` 自带调度、去重与数据管道，适合大型项目。

> ⚙️ 在实际研究中，网站防护策略会不断更新，因此持续优化爬取逻辑、维护代理与Cookie池尤为重要。

---

## 二、我的网页爬虫项目：微博、豆瓣与北大法宝

本部分介绍我在博士阶段开发或改进的三个爬虫项目，应用于舆情分析、政策文本研究及社会文化主题建模等研究任务。

---

### 1️⃣ 微博爬虫项目（热搜与博文抓取）

微博爬虫相关的开源框架已较为成熟。我以 “`weiboSpider`” 为关键词在 GitHub 搜索并克隆了基础项目，随后进行了个性化调整（包括 `Cookies` 配置、保存路径与字段结构修改）。

本项目原计划用于博士论文“**基于词向量的老年歧视话语识别**”研究，虽最终未纳入正式分析部分，但积累了丰富的技术经验。  
特别感谢我的师弟 [ghmzhu](https://ghmzhu.github.io/)，在 2024 年暑期期间给予了大量指导与支持。

- 🔗 项目地址：[微博爬虫（GitHub）](https://github.com/Lingjun-Liu/Web_scraping/weibo-search-master2)  
- 📖 借鉴来源：[程序员阿江的 CrawlerTutorial](https://github.com/NanmiCoder/CrawlerTutorial)

**功能概述**：
- 自动抓取每日微博热搜关键词；
- 根据热搜标题抓取对应博文；
- 支持关键词过滤与抓取频率设定。

---

### 2️⃣ 北大法宝爬虫项目（法律文献抓取）

该项目主要用于博士论文中“**新中国以来老年人相关政策法规的系统梳理**”。  
最终共爬取、清洗并整理出 **7,000+ 条** 法律与政策文献记录。

北大法宝的网页结构相对规整，为静态页面，因此解析逻辑较为简洁。  
在 `Cursor` 的辅助下完成脚本编写，但仍保留人工干预环节以应对验证码验证。

**项目特点**：
- 支持按关键词、类别分页抓取；
- 基于 `Selenium` 的半自动化运行；
- 验证码需人工配合。

---

### 3️⃣ 豆瓣小组爬虫项目（社群话题分析）

豆瓣小组的讨论氛围更集中、噪音更少，几乎无广告或灌水内容，非常适合社会学与文化研究。  
我以“独生子女与老年照护”为主题，抓取并分析相关讨论，用于课堂实验和 `Topic Modeling` 练习。

部分数据仍在后续研究中，暂未公开。  

**功能与实践**：
- 抓取小组讨论文本及元数据（时间、热度、用户信息等）；
- 支持按话题分页；
- 考虑运行时间成本，后续部分任务外包执行。

---


## ✍️ 三、反思与收获

在这些爬虫项目的开发与应用过程中，我有以下几点切实的体会与总结：

1. **中文互联网社区蕴含丰富的研究潜力，但技术支持尚不完善**  
   相比于 Twitter、Reddit 等国外平台普遍提供开放 API，中国的主流社交平台（如微博、豆瓣等）普遍缺乏对学术研究者友好的接口。这导致尽管中文网络社区拥有丰富的社会讨论与文本资源，研究者往往需要自行编写爬虫获取数据，增加了研究成本，也带来了更多伦理与合规风险。

2. **网络数据的代表性有限，需结合社会人口学视角审慎使用**  
   实际抓取数据过程中，我意识到多数平台缺乏对用户基本信息（如性别、地区、年龄）的结构化标注，这限制了对社会群体的全面刻画。同时，由于平台算法影响和使用门槛差异，样本也存在明显偏倚。因此，在使用这些数据进行社会研究时，必须明确其**代表性局限与结构性偏斜**。

3. **建议任务外包，但应确保源文件保存与分析可复现性**  
   虽然自己编写爬虫可以加深理解，但在面对规模较大或周期较长的爬取任务时，自行执行往往效率低下。因此，合理地将部分任务外包是一种节省精力的解决方案。但必须注意：  
   - **务必保留完整的源数据与代码文件**，保证后续分析具备可追溯性；  
   - **复查脚本与逻辑是否合理**，确保数据处理过程符合科研可复现性的基本要求。

> 💡 爬虫不仅是数据工具，也是一种理解平台机制、识别研究边界的路径。未来希望进一步探索其与社会研究方法之间的交叉价值。
